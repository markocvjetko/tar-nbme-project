{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f10cc59-2232-4619-ae34-819fc7ddca1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43340e11-1716-47d1-b4e9-46744325b5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, csv\n",
    "from ast import literal_eval\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1f6d217-44ea-41e4-a549-68287e4327a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "FEATS_PATH = 'data/features.csv'\n",
    "NOTES_PATH = 'data/patient_notes.csv'\n",
    "TRAIN_PATH = 'data/train.csv'\n",
    "OUTPUT_PATH = 'data/simplified/'\n",
    "\n",
    "def loc_list_to_ints(loc_list):\n",
    "    to_return = []\n",
    "    for loc_str in loc_list:\n",
    "        loc_strs = loc_str.split(\";\")\n",
    "        for loc in loc_strs:\n",
    "            start, end = loc.split()\n",
    "            to_return.append((int(start), int(end)))\n",
    "    return to_return\n",
    "\n",
    "\n",
    "def simplify_dataset():\n",
    "    '''\n",
    "    iz train.csv kakav je dan na kaggle natjecanju radi pojednostavljenu reprezentaciju podataka. od svakog patient notea stvara \n",
    "    csv datoteku pod imenom patientnum_casenum.csv\n",
    "    \n",
    "    stupci u csv datoteci su (token, lokacija, oznaka)\n",
    "    \n",
    "    Trenutno se tekst tokenizira po characterima, to mi se cinilo najjednostavnije, ali zbog toga je stupac \"lokacija\" trenutno redundantan.\n",
    "    \n",
    "    label -1 znaci da nema nijednog featurea na tokenu. \n",
    "    '''\n",
    "    df_train = pd.read_csv(TRAIN_PATH)\n",
    "    df_feats = pd.read_csv(FEATS_PATH)\n",
    "    df_notes = pd.read_csv(NOTES_PATH)\n",
    "    \n",
    "    df_train[\"location_list\"] = [literal_eval(x) for x in df_train[\"location\"]]\n",
    "        \n",
    "    id_feat_dict = dict(zip(df_feats.feature_num, df_feats.feature_text))\n",
    "    \n",
    "    unique_pn_nums = df_train.pn_num.unique()\n",
    "    print((unique_pn_nums.shape))\n",
    "    for pn_num in unique_pn_nums:\n",
    "        train = df_train.loc[df_train['pn_num'] == pn_num]\n",
    "        case_num = train.case_num.unique()[0]\n",
    "        pn_note = df_notes.loc[df_notes['pn_num'] == pn_num].values[0][2]\n",
    "        pn_note_tokens = [char for char in pn_note]\n",
    "        word_locs = []\n",
    "        curr_loc = 0\n",
    "        labels = [-1] * len(pn_note_tokens)\n",
    "        \n",
    "        for word in pn_note_tokens:\n",
    "            word_locs.append((curr_loc, curr_loc + len(word)))\n",
    "            curr_loc += len(word)\n",
    "        \n",
    "        for index, row in train.iterrows():\n",
    "            feat_num = row['feature_num']\n",
    "            \n",
    "            feat_locs = loc_list_to_ints(row['location_list'])\n",
    "            for l in feat_locs:\n",
    "                for i, w in enumerate(word_locs):\n",
    "                    if l[0] <= w[0] and l[1] >= w[1]:\n",
    "                        labels[i] = feat_num\n",
    "    \n",
    "        out_file = open(OUTPUT_PATH + str(case_num) + '_' +str(pn_num) + '.csv', 'w')\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(['word', 'location', 'label'])\n",
    "        for word, loc, label in zip(pn_note_tokens, word_locs, labels):\n",
    "            csv_writer.writerow([word, '[' + str(loc[0]) + ' ' + str(loc[1]) + ']', label])\n",
    "            \n",
    "        out_file.close()\n",
    "                \n",
    "simplify_dataset() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd9589f2-59a8-40ad-a94a-6ce8901489f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_id(pn_num, feature_num):\n",
    "    return str(pn_num)+'_'+str(feature_num)\n",
    "def parse_case_num(case_num):\n",
    "    return int(case_num)\n",
    "def parse_pn_num(pn_num):\n",
    "    return int(pn_num)\n",
    "def parse_feature_num(feature_num):\n",
    "    return int(feature_num)\n",
    "def parse_annotation(annotation):\n",
    "    if len(annotation) == 0:\n",
    "        return '[]'\n",
    "    return str(annotation)\n",
    "def parse_location(location):\n",
    "    if len(location) == 0:\n",
    "        return '[]'\n",
    "    parsed_loc = \"[\"\n",
    "    for i, loc in enumerate(location):\n",
    "        parsed_loc += \"'\" + str(loc[0]) + ' ' + str(loc[1])\n",
    "        if i != len(location) - 1:\n",
    "            parsed_loc += \"', \"\n",
    "        else:\n",
    "            parsed_loc += \"']\"\n",
    "    return parsed_loc\n",
    "            \n",
    "DATA_PATH = 'data/simplified/'\n",
    "\n",
    "def complexify_data():\n",
    "    '''\n",
    "    ucitava sve podatke u pojednostavljenom formatu (lista tokena i lista oznaka), parsira ih u zajednicki dataframe nalik onom u train.csv\n",
    "    '''\n",
    "    data_files = [f for f in listdir(DATA_PATH) if isfile(join(DATA_PATH, f))]\n",
    "    list_out = []\n",
    "    \n",
    "    #dict u kojem su kljucevi case_numovi a valuesi lista svih pripadajucih feature_numova \n",
    "    case_feat_dict = {}\n",
    "    df_feats = pd.read_csv(FEATS_PATH)\n",
    "    for case_num in df_feats.case_num.unique():\n",
    "        case_num_feats = df_feats[df_feats['case_num'] == case_num].feature_num.unique()\n",
    "        case_feat_dict[case_num] = list(case_num_feats) \n",
    "        #print(case_num, case_feat_dict[case_num])\n",
    "\n",
    "    col_names = ['id', 'case_num', 'pn_num', 'feature_num', 'annotation', 'location']\n",
    "    for file in data_files:\n",
    "        df = pd.read_csv(DATA_PATH + file)\n",
    "        #print(data_files[0])\n",
    "        case_num, pn_num = (int(i) for i in file[:-4].split('_'))\n",
    "        #print(pn_num, case_num)\n",
    "        df_feats = pd.read_csv(FEATS_PATH)\n",
    "        id_feat_dict = dict(zip(df_feats.feature_num, df_feats.feature_text))\n",
    "        \n",
    "        for feature_num in case_feat_dict[case_num]:\n",
    "        \n",
    "        #pretra≈æuje nalazi li se u oznakama tokena current feature. \n",
    "            feat_locs = []\n",
    "            curr_loc = []\n",
    "            annotation = []\n",
    "            i = 0\n",
    "\n",
    "            is_same = False\n",
    "            for word, loc, label in df.values:\n",
    "                loc = tuple([int(n) for n in loc[1:-1].split()])\n",
    "                if label == feature_num:\n",
    "                    if is_same == True:\n",
    "                        curr_loc = (curr_loc[0], loc[1])\n",
    "                    else:\n",
    "                        curr_loc = loc\n",
    "                        is_same = True\n",
    "                else:\n",
    "                    if is_same == True:\n",
    "                        feat_locs.append(curr_loc)\n",
    "                        annotation.append(''.join(df.values[:, 0][curr_loc[0]:curr_loc[1]]))\n",
    "                        is_same = False\n",
    "                    else:\n",
    "                        pass\n",
    "            row = []\n",
    "            row.append(parse_id(pn_num, feature_num))\n",
    "            row.append(parse_case_num(case_num))\n",
    "            row.append(parse_pn_num(pn_num))\n",
    "            row.append(parse_feature_num(feature_num))\n",
    "            row.append(parse_annotation(annotation))\n",
    "            row.append(parse_location(feat_locs))\n",
    "            list_out.append(row)\n",
    "\n",
    "        df_out = pd.DataFrame(list_out, columns=col_names)\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16334dea-ce48-454d-9f2e-8016aa1217d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESTORED_PATH = 'data/train_restored.csv'\n",
    "df_out = complexify_data()\n",
    "df_out.to_csv(RESTORED_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ece711-447d-47cf-9822-683035ff6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac519ef-53ee-422a-a800-ab9a6b129255",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(TRAIN_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151005b-6c99-4ed4-91ae-d9a2f1c405c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dtypes\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd50a6-0006-424d-a82c-d1387be69259",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.loc[df_test['pn_num'] == 71432]\n",
    "df_test.values[8, 5] == df_out.values[8, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d972fa-fd8a-4eb4-a18c-b66bc21bce8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c69529-d9ac-4ada-a459-ad5daa658608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b03d5-8daa-4969-85e9-76db6e3d9219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
