{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43340e11-1716-47d1-b4e9-46744325b5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, csv\n",
    "from ast import literal_eval\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afccd206-1eb3-4ad8-a211-6bebfafbd07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_list_to_ints(loc_list):\n",
    "    to_return = []\n",
    "    for loc_str in loc_list:\n",
    "        loc_strs = loc_str.split(\";\")\n",
    "        for loc in loc_strs:\n",
    "            start, end = loc.split()\n",
    "            to_return.append((int(start), int(end)))\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4934b048-1313-4857-a7ba-6ffc3f7fc22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTokenizer():\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return [char for char in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f6d217-44ea-41e4-a549-68287e4327a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO transform 3 lists into one list of tuples\n",
    "def simplify_dataset(train_path, features_path, pn_notes_path, tokenizer):\n",
    "    '''\n",
    "    #TODO transform 3 lists into one list of tuples\n",
    "    \n",
    "    Takes 3 .csv file paths which should be in the same form as the train.csv, features.csv and patient_notes.csv in the NBME kaggle competition. \n",
    "    Creates a simplified representation of the dataset which can later be used for data augmentation. The simplified data consists of 3 lists of\n",
    "    equal length.\n",
    "    \n",
    "    1st list contains a sequence of tokens, 2nd holds their location (start index, end index)\n",
    "    in the patient note,  3rd contains their labels.\n",
    "    \n",
    "    Label -1 denotes denotes that a token has no feature.\n",
    "    '''\n",
    "    df_train = pd.read_csv(train_path)\n",
    "    df_feats = pd.read_csv(features_path)\n",
    "    df_notes = pd.read_csv(pn_notes_path)\n",
    "    \n",
    "    df_train[\"location_list\"] = [literal_eval(x) for x in df_train[\"location\"]]\n",
    "    \n",
    "    simplified_data = []\n",
    "    \n",
    "    id_feat_dict = dict(zip(df_feats.feature_num, df_feats.feature_text))\n",
    "    unique_pn_nums = df_train.pn_num.unique()\n",
    "    for pn_num in unique_pn_nums:\n",
    "        train = df_train.loc[df_train['pn_num'] == pn_num]\n",
    "        case_num = train.case_num.unique()[0]\n",
    "        pn_note = df_notes.loc[df_notes['pn_num'] == pn_num].values[0][2]\n",
    "        tokens = tokenizer.tokenize(pn_note)\n",
    "        word_locs = []\n",
    "        curr_loc = 0\n",
    "        labels = [-1] * len(tokens)\n",
    "        \n",
    "        for word in tokens:\n",
    "            word_locs.append((curr_loc, curr_loc + len(word)))\n",
    "            curr_loc += len(word)\n",
    "        \n",
    "        for index, row in train.iterrows():\n",
    "            feat_num = row['feature_num']\n",
    "            \n",
    "            feat_locs = loc_list_to_ints(row['location_list'])\n",
    "            for l in feat_locs:\n",
    "                for i, w in enumerate(word_locs):\n",
    "                    if l[0] <= w[0] and l[1] >= w[1]:\n",
    "                        labels[i] = feat_num\n",
    "    \n",
    "        simplified_data.append((tokens, word_locs, labels, case_num, pn_num))\n",
    "    return simplified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61b5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complexify_data(data_path, features_path):\n",
    "    '''\n",
    "    Takes path of a folder with simplified data .csv files in it. Parses all simplified data in the folder into a single dataframe, \n",
    "    corresponding to train.csv from the NBME Kaggle competition\n",
    "    \n",
    "    ucitava sve podatke u pojednostavljenom formatu (lista tokena i lista oznaka), parsira ih u zajednicki dataframe nalik onom u train.csv\n",
    "    '''\n",
    "    data_files = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "    list_out = []\n",
    "    \n",
    "    #dict u kojem su kljucevi case_numovi a valuesi lista svih pripadajucih feature_numova \n",
    "    case_feat_dict = {}\n",
    "    df_feats = pd.read_csv(features_path)\n",
    "    for case_num in df_feats.case_num.unique():\n",
    "        case_num_feats = df_feats[df_feats['case_num'] == case_num].feature_num.unique()\n",
    "        case_feat_dict[case_num] = list(case_num_feats) \n",
    "#         print(case_num, case_feat_dict[case_num])\n",
    "\n",
    "    col_names = ['id', 'case_num', 'pn_num', 'feature_num', 'annotation', 'location']\n",
    "    for file in data_files:\n",
    "        df = pd.read_csv(data_path + file)\n",
    "        #print(data_files[0])\n",
    "        pn_num, case_num = (int(i) for i in file[:-4].split('_'))\n",
    "        #print(pn_num, case_num)\n",
    "        df_feats = pd.read_csv(features_path)\n",
    "        id_feat_dict = dict(zip(df_feats.feature_num, df_feats.feature_text))\n",
    "        \n",
    "        for feature_num in case_feat_dict[case_num]:\n",
    "        \n",
    "        #pretra≈æuje nalazi li se u oznakama tokena current feature. \n",
    "            feat_locs = []\n",
    "            curr_loc = []\n",
    "            annotation = []\n",
    "            i = 0\n",
    "\n",
    "            is_same = False\n",
    "            for word, loc, label in df.values:\n",
    "                loc = tuple([int(n) for n in loc[1:-1].split()])\n",
    "                if label == feature_num:\n",
    "                    if is_same == True:\n",
    "                        curr_loc = (curr_loc[0], loc[1])\n",
    "                    else:\n",
    "                        curr_loc = loc\n",
    "                        is_same = True\n",
    "                else:\n",
    "                    if is_same == True:\n",
    "                        feat_locs.append(curr_loc)\n",
    "                        annotation.append(''.join(df.values[:, 0][curr_loc[0]:curr_loc[1]]))\n",
    "                        is_same = False\n",
    "                    else:\n",
    "                        pass\n",
    "            row = []\n",
    "            row.append(parse_id(pn_num, feature_num))\n",
    "            row.append(parse_case_num(case_num))\n",
    "            row.append(parse_pn_num(pn_num))\n",
    "            row.append(parse_feature_num(feature_num))\n",
    "            row.append(parse_annotation(annotation))\n",
    "            row.append(parse_location(feat_locs))\n",
    "            list_out.append(row)\n",
    "\n",
    "        df_out = pd.DataFrame(list_out, columns=col_names)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe6b8f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_simplified_data(tokens, locations, labels, path):\n",
    "        \n",
    "        #file = open(OUTPUT_PATH + str(case_num) + '_' +str(pn_num) + '.csv', 'w')\n",
    "        file = open(path, 'w')\n",
    "        csv_writer = csv.writer(file)\n",
    "        csv_writer.writerow(['word', 'location', 'label'])\n",
    "        for word, loc, label in zip(tokens, locations, labels):\n",
    "            csv_writer.writerow([word, '[' + str(loc[0]) + ' ' + str(loc[1]) + ']', label])\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d89e2e8-30c9-42c6-91a2-df60207c48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_id(pn_num, feature_num):\n",
    "    return str(pn_num)+'_'+str(feature_num)\n",
    "\n",
    "def parse_case_num(case_num):\n",
    "    return int(case_num)\n",
    "\n",
    "def parse_pn_num(pn_num):\n",
    "    return int(pn_num)\n",
    "\n",
    "def parse_feature_num(feature_num):\n",
    "    return int(feature_num)\n",
    "\n",
    "def parse_annotation(annotation):\n",
    "    if len(annotation) == 0:\n",
    "        return '[]'\n",
    "    return str(annotation)\n",
    "\n",
    "def parse_location(location):\n",
    "    if len(location) == 0:\n",
    "        return '[]'\n",
    "    parsed_loc = \"[\"\n",
    "    for i, loc in enumerate(location):\n",
    "        parsed_loc += \"'\" + str(loc[0]) + ' ' + str(loc[1])\n",
    "        if i != len(location) - 1:\n",
    "            parsed_loc += \"', \"\n",
    "        else:\n",
    "            parsed_loc += \"']\"\n",
    "    return parsed_loc  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
