{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed88665-f597-4369-ac6d-1aacc06cffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, csv\n",
    "from ast import literal_eval\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7683d96d-88c6-458c-8d1b-0d11c03c0319",
   "metadata": {},
   "source": [
    "Splits the original train dataset into train and validation.\n",
    "Additionally, only takes the cases specified by CASE_NUMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f6ff22c-3109-455a-92f5-7d44ffb37e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'dataset/original/train.csv'\n",
    "PN_NOTES_PATH = 'dataset/original/pn_notes.csv'\n",
    "FEATURES_PATH = 'dataset/original/features.csv'\n",
    "\n",
    "SUBSAMPLED_TRAIN_PATH = 'dataset/subset_250/train.csv'\n",
    "SUBSAMPLED_VALID_PATH = 'dataset/subset_250/valid.csv'\n",
    "SUBSAMPLED_FEATURES_PATH = 'dataset/subset_250/features.csv'\n",
    "SUBSAMPLED_PN_NOTES_PATH = 'dataset/subset_250/pn_notes.csv'\n",
    "\n",
    "CASE_NUMS = [0, 1, 2, 3, 4]\n",
    "VALID_NUM_PNS_PER_CASE = 30 \n",
    "TRAIN_NUM_PNS_PER_CASE = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f3805ac-00a0-4696-ba68-e63d1ddcc770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6510, 6) (390, 6)\n",
      "(6120, 6) (390, 6)\n",
      "(5610, 6) (510, 6)\n",
      "(5130, 6) (480, 6)\n",
      "(4830, 6) (300, 6)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "df = df.loc[df['case_num'] <= 4]\n",
    "\n",
    "df_valid_list = []\n",
    "for case_num in CASE_NUMS:\n",
    "    df_case = df.loc[df['case_num'] == case_num]\n",
    "    pn_nums = list(df_case.pn_num.unique())\n",
    "    pn_nums =random.sample(pn_nums, VALID_NUM_PNS_PER_CASE)\n",
    "    df_case = df.loc[df['pn_num'].isin(pn_nums)]\n",
    "    df = df.drop(df.index[df['pn_num'].isin(pn_nums)]) #removing selected validation samples from the train dataset\n",
    "    print(df.shape, df_case.shape)\n",
    "    df_valid_list.append(df_case)\n",
    "\n",
    "    \n",
    "df_valid = pd.concat(df_valid_list, axis=0)\n",
    "df_valid.to_csv(SUBSAMPLED_VALID_PATH, index=False)\n",
    "\n",
    "df_train_subset = []\n",
    "for case_num in CASE_NUMS:\n",
    "    df_case = df.loc[df['case_num'] == case_num]\n",
    "    pn_nums = list(df_case.pn_num.unique())\n",
    "    pn_nums =random.sample(pn_nums, TRAIN_NUM_PNS_PER_CASE)\n",
    "    df_case = df.loc[df['pn_num'].isin(pn_nums)]\n",
    "    df_train_subset.append(df_case)\n",
    "    df_train = df.drop(df.index[df['pn_num'].isin(pn_nums)]) #removing selected train samples\n",
    "    \n",
    "df_train_subset = pd.concat(df_train_subset)\n",
    "df_train_subset.to_csv(SUBSAMPLED_TRAIN_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e2f28d4-9bbf-46ca-8070-e18c0867bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_features = pd.read_csv(FEATURES_PATH)\n",
    "df_features = df_features.loc[df_features['case_num'] <= 4]\n",
    "df_features.to_csv(SUBSAMPLED_FEATURES_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
