{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b64e5fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from augmentation import *\n",
    "from parsing import *\n",
    "from subsampling import *\n",
    "import os\n",
    "from collections import Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36581a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd() + '/tar-project/'\n",
    "PATIENT_NOTES_PATH = PATH + 'patient_notes.csv'\n",
    "FEATS_PATH = PATH + 'features.csv'\n",
    "TRAIN_PATH = PATH + 'train.csv'\n",
    "TEST_PATH = PATH + 'test.csv'\n",
    "VALID_PATH = PATH + 'valid.csv'\n",
    "\n",
    "\n",
    "df_patient_notes = pd.read_csv(PATIENT_NOTES_PATH)\n",
    "df_features = pd.read_csv(FEATS_PATH)\n",
    "df_train = pd.read_csv(TRAIN_PATH)\n",
    "df_test = pd.read_csv(TEST_PATH)\n",
    "df_valid = pd.read_csv(VALID_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc109bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique features: 84\n"
     ]
    }
   ],
   "source": [
    "def count(row):\n",
    "    return row[2].split('OR')\n",
    "\n",
    "df_features = df_features.loc[df_features['case_num'].isin([0, 1, 2, 3, 4])]\n",
    "df_features['num_of_features'] = df_features.apply(count, axis=1)\n",
    "\n",
    "s = set()\n",
    "for l in df_features['num_of_features'].tolist():\n",
    "    for feature in l:\n",
    "        s.add(feature.lower())\n",
    "print('Unique features: ', end='')\n",
    "print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c3f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_all_features(df):\n",
    "    \n",
    "    def count(row):\n",
    "        if row[5] == '[]':\n",
    "            return 0\n",
    "        return len(row[5].split(','))\n",
    "    \n",
    "    df = df.loc[df['case_num'].isin([0, 1, 2, 3, 4])]\n",
    "    df['num_of_location'] = df.apply(count, axis=1)\n",
    "    \n",
    "    return df['num_of_location'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a9cdac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features train: 2347\n",
      "All features test: 1284\n",
      "All features valid: 1820\n"
     ]
    }
   ],
   "source": [
    "print('All features train: ', end='')\n",
    "print(get_num_all_features(df_train))\n",
    "print('All features test: ', end='')\n",
    "print(get_num_all_features(df_test))\n",
    "print('All features valid: ', end='')\n",
    "print(get_num_all_features(df_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50e1394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tokens(df):\n",
    "    \n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    df_pn = df_patient_notes.loc[df_patient_notes['pn_num'].isin(df['pn_num'].tolist())]\n",
    "    sentences = df_pn['pn_history'].tolist()\n",
    "    instances = [tokenizer.tokenize(sentence.lower()) for sentence in sentences]\n",
    "    sentences_clean = []\n",
    "    \n",
    "    for sentence in instances:\n",
    "        for word in sentence:\n",
    "            sentences_clean.append(word.lower())\n",
    "    freq = dict(Counter(sentences_clean))\n",
    "\n",
    "    \n",
    "    return sum(freq.values()), len(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf3a9341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tokens train: 27182 \n",
      "Unique tokens train: 2383\n",
      "\n",
      "All tokens test: 14259 \n",
      "Unique tokens test: 1708\n",
      "\n",
      "All tokens valid: 21008 \n",
      "Unique tokens valid: 2062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a, u = get_all_tokens(df_train)\n",
    "print('All tokens train: {} \\nUnique tokens train: {}\\n'.format(a, u))\n",
    "a, u = get_all_tokens(df_test)\n",
    "print('All tokens test: {} \\nUnique tokens test: {}\\n'.format(a, u))\n",
    "a, u = get_all_tokens(df_valid)\n",
    "print('All tokens valid: {} \\nUnique tokens valid: {}\\n'.format(a, u))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
