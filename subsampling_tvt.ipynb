{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed88665-f597-4369-ac6d-1aacc06cffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, csv\n",
    "from ast import literal_eval\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7683d96d-88c6-458c-8d1b-0d11c03c0319",
   "metadata": {},
   "source": [
    "Splits the original train dataset into train and validation.\n",
    "Additionally, only takes the cases specified by CASE_NUMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f6ff22c-3109-455a-92f5-7d44ffb37e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'dataset/original/train.csv'\n",
    "PN_NOTES_PATH = 'dataset/original/patient_notes.csv'\n",
    "FEATURES_PATH = 'dataset/original/features.csv'\n",
    "\n",
    "SUBSAMPLED_TRAIN_PATH = 'dataset/subset_250/train.csv'\n",
    "SUBSAMPLED_VALID_PATH = 'dataset/subset_250/valid.csv'\n",
    "SUBSAMPLED_TEST_PATH = 'dataset/subset_250/test.csv'\n",
    "SUBSAMPLED_FEATURES_PATH = 'dataset/subset_250/features.csv'\n",
    "SUBSAMPLED_PN_NOTES_PATH = 'dataset/subset_250/patient_notes.csv'\n",
    "\n",
    "CASE_NUMS = [0, 1, 2, 3, 4]\n",
    "VALID_NUM_PNS_PER_CASE = 20 \n",
    "TRAIN_NUM_PNS_PER_CASE = 60\n",
    "TEST_NUM_PNS_PER_CASE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f3805ac-00a0-4696-ba68-e63d1ddcc770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "df = df.loc[df['case_num'] <= 4]\n",
    "\n",
    "df_valid_list = []\n",
    "df_test_list = []\n",
    "for case_num in CASE_NUMS:\n",
    "    df_case = df.loc[df['case_num'] == case_num]\n",
    "    pn_nums = list(df_case.pn_num.unique())\n",
    "    pn_nums =random.sample(pn_nums, VALID_NUM_PNS_PER_CASE + TEST_NUM_PNS_PER_CASE)\n",
    "    pn_nums_val = pn_nums[0:20]\n",
    "    pn_nums_test = pn_nums[20:]\n",
    "    # za val\n",
    "    df_case = df.loc[df['pn_num'].isin(pn_nums_val)]\n",
    "    df = df.drop(df.index[df['pn_num'].isin(pn_nums_val)]) #removing selected validation samples from the train dataset\n",
    "    df_valid_list.append(df_case)\n",
    "    # za test\n",
    "    df_case = df.loc[df['pn_num'].isin(pn_nums_test)]\n",
    "    df = df.drop(df.index[df['pn_num'].isin(pn_nums_test)]) #removing selected validation samples from the train dataset\n",
    "    df_test_list.append(df_case)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "df_valid = pd.concat(df_valid_list, axis=0)\n",
    "df_valid.to_csv(SUBSAMPLED_VALID_PATH, index=False)\n",
    "\n",
    "df_test = pd.concat(df_test_list, axis=0)\n",
    "df_test.to_csv(SUBSAMPLED_TEST_PATH, index=False)\n",
    "\n",
    "df_train_subset = []\n",
    "for case_num in CASE_NUMS:\n",
    "    df_case = df.loc[df['case_num'] == case_num]\n",
    "    pn_nums = list(df_case.pn_num.unique())\n",
    "    pn_nums =random.sample(pn_nums, TRAIN_NUM_PNS_PER_CASE)\n",
    "    df_case = df.loc[df['pn_num'].isin(pn_nums)]\n",
    "    df_train_subset.append(df_case)\n",
    "    df_train = df.drop(df.index[df['pn_num'].isin(pn_nums)]) #removing selected train samples\n",
    "    \n",
    "df_train_subset = pd.concat(df_train_subset)\n",
    "df_train_subset.to_csv(SUBSAMPLED_TRAIN_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e2f28d4-9bbf-46ca-8070-e18c0867bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_features = pd.read_csv(FEATURES_PATH)\n",
    "df_features = df_features.loc[df_features['case_num'] <= 4]\n",
    "df_features.to_csv(SUBSAMPLED_FEATURES_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc47168-7511-4323-90f5-fe7fe14ea0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_subset = pd.read_csv(SUBSAMPLED_TRAIN_PATH)\n",
    "df_valid_subset = pd.read_csv(SUBSAMPLED_VALID_PATH)\n",
    "df_test_subset = pd.read_csv(SUBSAMPLED_TEST_PATH)\n",
    "\n",
    "train_unique = df_train_subset['pn_num'].unique()\n",
    "valid_unique = df_valid_subset['pn_num'].unique()\n",
    "test_unique = df_test_subset['pn_num'].unique()\n",
    "\n",
    "unique = np.concatenate((train_unique, valid_unique, test_unique))\n",
    "\n",
    "\n",
    "df_pn_notes = pd.read_csv(PN_NOTES_PATH)\n",
    "df_pn_notes = df_pn_notes.loc[df_pn_notes['pn_num'].isin(unique)]\n",
    "df_pn_notes.to_csv(SUBSAMPLED_PN_NOTES_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451270d-0e9a-48c0-a54d-7e682b1afd40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a070fbf3-65c1-499a-a61e-3707105031bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
