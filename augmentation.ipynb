{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c31f0cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "import math\n",
    "from enum import Enum\n",
    "import copy\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d790c4-e876-4310-9ab4-cec98702573c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeatureSequence():\n",
    "    \n",
    "    def __init__(self, feature_num, start_loc, end_loc):\n",
    "        self.feature_num = feature_num\n",
    "        self.start_loc = start_loc\n",
    "        self.end_loc = end_loc\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.end_loc - self.start_loc\n",
    "\n",
    "class Sample():\n",
    "    \n",
    "    def __init__(self, text, labels, case_num, pn_num):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.case_num = case_num\n",
    "        self.pn_num = pn_num\n",
    "        self.feat_seqs = []\n",
    "        self.update_feat_seqs()\n",
    "        \n",
    "    def update_feat_seqs(self):\n",
    "        self.feat_seqs = []\n",
    "        prev_label = self.labels[0]\n",
    "        start_loc = 0\n",
    "        end_loc = 0\n",
    "        for i, label in enumerate(self.labels):\n",
    "            if prev_label != label:\n",
    "                end_loc = i\n",
    "                self.feat_seqs.append(FeatureSequence(prev_label, start_loc, end_loc))\n",
    "                prev_label = label\n",
    "                start_loc = i\n",
    "        end_loc = i\n",
    "        self.feat_seqs.append(FeatureSequence(prev_label, start_loc, end_loc))\n",
    "    \n",
    "    def get_num_word(self):\n",
    "        return len(self.text.split())\n",
    "    \n",
    "    def feat_seq_num(self):\n",
    "        return len(self.feat_seqs)\n",
    "    \n",
    "    def select_random_feat_seq(self, feature_num=None, min_length=10):\n",
    "        filtered_seqs = [seq for seq in self.feat_seqs if len(seq) >= 5]\n",
    "        if feature_num != None:\n",
    "            filtered_seqs = [seq for seq in filtered_seqs if seq.feature_num == feature_num]\n",
    "        return random.choice(filtered_seqs)   \n",
    "    \n",
    "    def select_random_word(self):\n",
    "        text = ''.join(self.text)\n",
    "        words = text.split()\n",
    "        word = random.choice(words)\n",
    "        loc = text.find(word)\n",
    "        return word, loc\n",
    "        \n",
    "        \n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)\n",
    "    \n",
    "\n",
    "class AugmenterType(Enum):\n",
    "    WORD = 1\n",
    "    SEQUENCE = 2\n",
    "\n",
    "class ShuffleSequenceAugmenter():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.type = AugmenterType.SEQUENCE\n",
    "        \n",
    "    def augment(self, text):\n",
    "        words = text.split()\n",
    "        words = random.shuffle(words)\n",
    "        return ' '.join(words)\n",
    "\n",
    "class BackTranslationAugmenter():\n",
    "    def __init__(self):\n",
    "        self.type = AugmenterType.SEQUENCE\n",
    "        self.augmenter = naw.BackTranslationAug(\n",
    "            from_model_name='facebook/wmt19-en-de', \n",
    "            to_model_name='facebook/wmt19-de-en')\n",
    "    \n",
    "    def augment(self, text):\n",
    "        return self.augmenter.augment(text)\n",
    "\n",
    "class SynonymAugmenter():\n",
    "    def __init__(self):\n",
    "        self.type = AugmenterType.WORD\n",
    "        self.augmenter = naw.SynonymAug(aug_src='wordnet')\n",
    "        \n",
    "    def augment(self, text):\n",
    "        return self.augmenter.augment(text)\n",
    "\n",
    "        \n",
    "def augment(augmenter, sample, percentage_to_augment = 0.1, min_length=10):\n",
    "    if augmenter.type == AugmenterType.WORD:\n",
    "        words_to_augment = math.ceil(sample.get_num_word() * percentage_to_augment)\n",
    "        for i in range(words_to_augment):\n",
    "            word, loc = sample.select_random_word()\n",
    "            aug_word = augmenter.augment(word)\n",
    "#             print(word, aug_word)\n",
    "            if len(word) < len(aug_word) :\n",
    "#                 print('adding', len(aug_word) - len(word))\n",
    "                for j in range(len(aug_word) - len(word)):\n",
    "                    sample.labels.insert(loc, sample.labels[loc])\n",
    "            if len(word) > len(aug_word):\n",
    "#                 print('removing', len(word) - len(aug_word))\n",
    "                for j in range(len(word) - len(aug_word)):\n",
    "#                     print('inside')\n",
    "                    sample.labels.pop(loc)\n",
    "            sample.text = sample.text.replace(word, aug_word, 1)\n",
    "        sample.update_feat_seqs()\n",
    "    \n",
    "    if augmenter.type == AugmenterType.SEQUENCE:\n",
    "        sequences_to_augment = math.ceil(sample.feat_seq_num() * percentage_to_augment)\n",
    "#         print('seqs to augment', sequences_to_augment)\n",
    "        for i in range(sequences_to_augment):\n",
    "            \n",
    "            seq = sample.select_random_feat_seq()\n",
    "            tokens = sample.text[seq.start_loc:seq.end_loc]\n",
    "            \n",
    "            prefix_whitespace = False\n",
    "            postfix_whitespace = False\n",
    "            if tokens[0] == ' ':\n",
    "                prefix_whitespace = True\n",
    "            if tokens[-1] == ' ':\n",
    "                postfix_whitespace = True\n",
    "                \n",
    "            aug_tokens = augmenter.augment(tokens)\n",
    "            if aug_tokens == '':\n",
    "                aug_tokens = tokens\n",
    "            if len(tokens) < len(aug_tokens) :\n",
    "#                 print('adding', len(aug_tokens) - len(tokens))\n",
    "                for j in range(len(aug_tokens) - len(tokens)):\n",
    "                    sample.labels.insert(seq.start_loc, seq.feature_num)\n",
    "            if len(tokens) > len(aug_tokens):\n",
    "#                 print('removing', len(tokens) - len(aug_tokens))\n",
    "                for j in range(len(tokens) - len(aug_tokens)):\n",
    "#                     print('inside')\n",
    "                    sample.labels.pop(seq.start_loc)\n",
    "#             print(tokens)\n",
    "#             print(aug_tokens)\n",
    "            \n",
    "            if prefix_whitespace:\n",
    "#                 print('in_prefix')\n",
    "                aug_tokens = aug_tokens + ' '\n",
    "                sample.labels.insert(seq.start_loc, seq.feature_num)\n",
    "            \n",
    "            if postfix_whitespace:\n",
    "#                 print('in postfix')\n",
    "                aug_tokens = ' ' + aug_tokens\n",
    "                sample.labels.insert(seq.start_loc, seq.feature_num)\n",
    "                \n",
    "            sample.text = sample.text.replace(tokens, aug_tokens, 1)\n",
    "            sample.update_feat_seqs()\n",
    "            \n",
    "def read_simplified_data(path):\n",
    "    \n",
    "    data_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    samples = []\n",
    "    for data_file in data_files:\n",
    "        case_num, pn_num = (int(i) for i in data_file[:-4].split('_'))\n",
    "        df = pd.read_csv(path + data_file)  \n",
    "        tokens = ''.join(df.word.to_list())\n",
    "        labels = df.label.to_list()\n",
    "        samples.append(Sample(tokens, labels, case_num, pn_num))\n",
    "    return samples\n",
    "\n",
    "def augment_simplified_data(num_augmentations, percentage_to_augment, augmenter, sample, min_length):\n",
    "    \n",
    "    augmented_samples = []\n",
    "    for i in range(num_augmentations):\n",
    "        augmented_samples.append(sample.copy())\n",
    "    for i, aug_sample in enumerate(augmented_samples):\n",
    "        augment(augmenter, aug_sample, percentage_to_augment, min_length)\n",
    "        aug_sample.pn_num = (i+1)*10000000 + aug_sample.pn_num * sample.pn_num # creates a unique patient num\n",
    "    \n",
    "    return augmented_samples    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
